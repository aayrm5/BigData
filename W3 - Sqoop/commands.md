# Commands to be run on the Linux Terminal

## To check list of databases available in MYSQL
### Cloudera's quickstart VM could be used.

```sh
sqoop-list-databases \
--connect "jdbc:mysql://quickstart.cloudera:3306" \
--username <username> \
--password <password> or -P
```

*`-P` prompts for password if you don't want to type it in the command. Instead of `quickstart.cloudera`, you could write `localhost` or `ip_address`*

## To check the list of tables:

```sh
sqoop-list-tables \
--connect "jdbc:mysql://quickstart.cloudera:3306/<db_name>" \
--username <username> \
--password <password>
```

## Use `EVAL` to run a query

```sh
sqoop-eval \
--connect "jdbc:mysql://<iP_address>:3306/<db_name>" \
--username <username> \
--password <password> \
--query "SELECT * FROM <dbName.TableName> LIMIT 10"
```

*Instead of `--query`, `-e` could also be used to define a query*

## Import data from RDBMS to Hadoop

- Transfers data from your RDBMS to HDFS.
- MapReduce job where only mappers work and **NO REDUCER**.
- By default, **4 Mappers** do the work by determining the split size based on the primary key.

```sh
sqoop-import \
--connect "jdbc:mysql://localhost:3306/<db_name>" \
--username <username> \
--password <password> \
--table <table_name> \
--num-mappers 1 \
--target-dir <directory_in_HDFS>
```

### Important points to note:
- When there is no `primary-key` specified for the table, explicitly set the `number of mappers` to 1 (-m 1 or --num-mappers 1) after mentioning the table name; Or mention a switch called `--split-by <column_name>` to partition the data upon. **If none of them are mentioned, the job would fail**.
  
- When `--target-dir` is specified in the sqoop cmd, the directory path mentioned will be the final path where the data is imported.

    The target directory specified in the command **SHOULD NOT EXIST**; the `sqoop-import` cmd will create this directory. If the path exists, the job will fail.

- Instead, if `--warehouse-dir` is specified, the system will create a sub-directory with the source table name under the path specified. *This is the suggested approach.*
  
    `<path_to_data> = <path_specified>/<Table_name>`

- `sqoop-import-all-tables` to import all the tables in the database.

## Supported file formats:

Sqoop supports 4 types of file formats:

- Text File (default) `--as-textfile`
- Sequential file `--as-sequencefile`
- Avro File `--as-avrodatafile`
- Parquet file `--as-parquetfile`

## Redirecting logs generated by the sqoop cmd output:

```sh
sqoop-import \
--connect "jdbc:mysql://localhost:3306/<db_name>" \
--username <username> \
--password <password> \
--table <table_name> \
--warehouse-dir </path_in_HDFS> \
1>query.out \
2>query.err
```
*Note: `query.out` contains warnings; `query.err` contains error logs*

## Concept of Boundary Query / BoundingValsQuery:

**This Boundary Query determines the Max & Min of the Primary Key**
Later, the split size is calculated based on the Max & Min values.

### Formula for Split Size:
`Split Size = (Max of PK - Min of PK) / Number of Mappers`

**Each Mapper is given this split size number of records to process. Thus parallelizing the job**.

## Sqoop import execution flow:

### How do mappers divide the work when a query is fired?

- Select 1 record and determine the structure of the table (getting metadata) and build a Java file. (SELECT t.* FROM <table_name> as t LIMIT 1;)
- Using the above Java file, sqoop builds a JAR file.
- BoundingValsQuery is determined based on the Min & Max of the Primary Key.
- BVQ runs the query `SELECT MIN(PK), MAX(PK) FROM <table_name>;`
- Calculate the Split Size. `(Max of PK - Min of PK) / Number of Mappers`. 4 mappers by default.

## Setting delimiters while importing

```sh
sqoop-import \
--connect "jdbc:mysql://localhost:3306/<db_name>" \
--username <username> \
--password <pw> \
--table <table_name> \
--fields-terminated-by '|' \
--lines-terminated-by ';' \
--target-dir <path_in_HDFS>
```

## Creating Hive Tables from Sqoop:

```sh
sqoop-import \ 
--connect "jdbc:mysql://localhost:3306/<db_name>" \
--username <uname> \
--password <pw> \
--table <table_in_RDBMS> \
--hive-table <hive_tableName> \
--fields-terminated-by '|'
```

Few Addtional Switches for Hive Job:
```sh
--verbose (to give detailed info on running job)
--append (to add more data into an existing directory)
--delelte-target-dir (to delete the directory of the target if it exists)
```

## Dealing with missing / Null values while importing data:

The following switches replace the null values for string / Numerical data:

```sh
--null-non-string "-1"
--null-string "other"
```

